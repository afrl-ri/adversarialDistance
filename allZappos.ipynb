{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate UT-Zap50K results\n",
    "\n",
    "Distribution A. Approved for public release: distribution unlimited.  Case 88ABW-2019-1334. 27 March 2019.\n",
    "\n",
    "THIS SOFTWARE AND ANY ACCOMPANYING DOCUMENTATION IS    \n",
    "RELEASED \"AS IS.\" THE US GOVERNMENT MAKES NO WARRANTY  \n",
    "OF ANY KIND, EXPRESS OR IMPLIED, CONCERNING THIS SOFTWARE  \n",
    "AND ANY ACCOMPANYING DOCUMENTATION, INCLUDING,  \n",
    "WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY   \n",
    "OR FITNES FOR A PARTICULAR PURPOSE. IN NO EVENT WILL THE  \n",
    "US GOVERNMENT BE LIABLE FOR ANY DAMAGES, INCLUDING ANY  \n",
    "LOST PROFITS, LOST SAVINGS, OR OTHER INCIDENTAL OR  \n",
    "CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE, OR  \n",
    "INABILITY TO USE, THIS SOFTWARE OR ANY ACCOMPANYING   \n",
    "DOCUMENTATION, EVEN IF INFORMED IN ADVANCE OF THE  \n",
    "POSSIBILITY OF SUCH DAMAGES.    \n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Environment\n",
    "##\n",
    "import os\n",
    "from tool import *\n",
    "from unkunk_utility import *\n",
    "from clustering import *\n",
    "from optim import *\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(1)\n",
    "import foolbox\n",
    "import numpy as np\n",
    "import keras\n",
    "import random\n",
    "import math\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.layers import Input, Dense, Activation, Conv2D, MaxPooling2D, Flatten, regularizers, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from IPython.display import HTML, display\n",
    "from foolbox.criteria import TargetClass\n",
    "from foolbox.distances import MAE\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import statsmodels.api as sm\n",
    "\n",
    "##\n",
    "## Model params\n",
    "##\n",
    "boundarySteps = 1000\n",
    "\n",
    "##\n",
    "## Unknown search params\n",
    "##  \n",
    "criticalClass = 0\n",
    "sampleSize = 2000\n",
    "toSelect = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zappos\n",
    "\n",
    "### Build model\n",
    "\n",
    "- Make dataset \n",
    "- Load data\n",
    "- Fit model\n",
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Make dataset (Creates numpy arrays for the zappos datasets)\n",
    "##\n",
    "# bootPath = 'ut-zap50k-images-square/Boots/'\n",
    "# sandalPath = 'ut-zap50k-images-square/Sandals/'\n",
    "# shoePath = 'ut-zap50k-images-square/Shoes/'\n",
    "# slipperPath = 'ut-zap50k-images-square/Slippers/'\n",
    "# ROW = 64\n",
    "# COL = 64\n",
    "# zapposDatasetMaker(bootPath, sandalPath, shoePath, slipperPath, ROW, COL)\n",
    "\n",
    "##\n",
    "## Load dataset\n",
    "##\n",
    "trainImages = np.load('trainImages.npy')\n",
    "trainLabels = np.load('trainLabels.npy')\n",
    "valImages = np.load('valImages.npy')\n",
    "valLabels = np.load('valLabels.npy')\n",
    "testImages = np.load('testImages.npy')\n",
    "testLabels = np.load('testLabels.npy')   \n",
    "\n",
    "##\n",
    "## Want not-shoe vs shoes (remove boots because they are way too easy to find)\n",
    "##\n",
    "trainDel = np.where(trainLabels == 0)    ## Remove easy to find boots\n",
    "valDel = np.where(valLabels == 0)        ## Remove easy to find boots\n",
    "testDel = np.where(testLabels == 0)      ## Remove easy to find boots\n",
    "\n",
    "trainImages = np.delete(trainImages, trainDel, axis = 0)\n",
    "trainLabels = np.delete(trainLabels, trainDel)\n",
    "\n",
    "valImages = np.delete(valImages, valDel, axis = 0)\n",
    "valLabels = np.delete(valLabels, valDel)\n",
    "\n",
    "testImages = np.delete(testImages, testDel, axis = 0)\n",
    "testLabels = np.delete(testLabels, testDel)\n",
    "\n",
    "trainLabels[trainLabels != 2] = 0      ## Recode to zero and one\n",
    "trainLabels[trainLabels == 2] = 1      ## Recode to zero and one\n",
    "\n",
    "valLabels[valLabels != 2] = 0          ## Recode to zero and one\n",
    "valLabels[valLabels == 2] = 1          ## Recode to zero and one\n",
    "\n",
    "testLabels[testLabels != 2] = 0        ## Recode to zero and one\n",
    "testLabels[testLabels == 2] = 1        ## Recode to zero and one\n",
    "\n",
    "##\n",
    "## Fit model\n",
    "##\n",
    "model = modelMaker(64, 64, 3, 2)\n",
    "model.fit(trainImages,\n",
    "           keras.utils.to_categorical(trainLabels),\n",
    "           batch_size = 128,\n",
    "           epochs=75,\n",
    "           verbose = 2)\n",
    "model.save('zappos.h5')\n",
    "model = load_model('zappos.h5')\n",
    "\n",
    "##\n",
    "## Evaluate\n",
    "##\n",
    "trainAcc = model.evaluate(trainImages, keras.utils.to_categorical(trainLabels), verbose=0)[1]\n",
    "predictedTrain = np.argmax(model.predict(trainImages), axis = 1)\n",
    "confidenceTrain = np.max(model.predict(trainImages), axis = 1)\n",
    "trainECE = ECE(confidenceTrain, predictedTrain, trainLabels, 15)\n",
    "\n",
    "valAcc = model.evaluate(valImages, keras.utils.to_categorical(valLabels), verbose=0)[1]\n",
    "predictedVal = np.argmax(model.predict(valImages), axis = 1)\n",
    "confidenceVal = np.max(model.predict(valImages), axis = 1)\n",
    "valECE = ECE(confidenceVal, predictedVal, valLabels, 15)\n",
    "\n",
    "testAcc = model.evaluate(testImages, keras.utils.to_categorical(testLabels), verbose=0)[1]\n",
    "predictedTest = np.argmax(model.predict(testImages), axis = 1)\n",
    "confidenceTest = np.max(model.predict(testImages), axis = 1)\n",
    "testECE = ECE(confidenceTest, predictedTest, testLabels, 15)\n",
    "\n",
    "table = [['Measure', 'Acc', 'ECE'],\n",
    "         ['Train', round(trainAcc, 2), round(trainECE, 2)],\n",
    "         ['Val', round(valAcc, 2), round(valECE, 2)],\n",
    "         ['Test', round(testAcc, 2), round(testECE, 2)]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "reliabilityDiagram(confidenceTest, predictedTest, testLabels, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This model is not calibrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate model\n",
    "\n",
    "- Calibrate model\n",
    "- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Calibrate\n",
    "##\n",
    "logitExtractor = logitExtract(model, 'output')\n",
    "calibrate = calibrateModel(logitExtractor, valImages, valLabels, classes = 2, epochs = 500)\n",
    "\n",
    "##\n",
    "## Extract values\n",
    "##\n",
    "trainAcc = model.evaluate(trainImages, keras.utils.to_categorical(trainLabels), verbose=0)[1]\n",
    "predictedTrain = np.argmax(model.predict(trainImages), axis = 1)\n",
    "confidenceTrain = np.max(calibrate.predict(logitExtractor.predict(trainImages)), axis = 1)\n",
    "trainECE = ECE(confidenceTrain, predictedTrain, trainLabels, 15)\n",
    "\n",
    "valAcc = model.evaluate(valImages, keras.utils.to_categorical(valLabels), verbose=0)[1]\n",
    "predictedVal = np.argmax(model.predict(valImages), axis = 1)\n",
    "confidenceVal = np.max(calibrate.predict(logitExtractor.predict(valImages)), axis = 1)\n",
    "valECE = ECE(confidenceVal, predictedVal, valLabels, 15)\n",
    "\n",
    "testAcc = model.evaluate(testImages, keras.utils.to_categorical(testLabels), verbose=0)[1]\n",
    "predictedTest = np.argmax(model.predict(testImages), axis = 1)\n",
    "confidenceTest = np.max(calibrate.predict(logitExtractor.predict(testImages)), axis = 1)\n",
    "testECE = ECE(confidenceTest, predictedTest, testLabels, 15)\n",
    "\n",
    "table = [['Measure', 'Acc', 'ECE'],\n",
    "         ['Train', round(trainAcc, 2), round(trainECE, 2)],\n",
    "         ['Val', round(valAcc, 2), round(valECE, 2)],\n",
    "         ['Test', round(testAcc, 2), round(testECE, 2)]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "reliabilityDiagram2(confidenceTest, predictedTest, testLabels, 15, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for high confidence errors\n",
    "\n",
    "1. Calculate distances\n",
    "2. Search for errors\n",
    "\n",
    "#### Create features for derived feature space\n",
    "\n",
    "- Consistent with previous literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = testImages.reshape(testImages.shape[0], testImages.shape[1] * testImages.shape[2] * testImages.shape[3])\n",
    "x = StandardScaler().fit_transform(features)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "principalComponents = np.round(pca.fit_transform(x), 4)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['P1', 'P2', 'P3', 'P4', 'P5'])\n",
    "\n",
    "df = pd.DataFrame({'Prediction': predictedTest,\n",
    "                   'Confidence': confidenceTest,\n",
    "                   'True Label': testLabels,\n",
    "                   'X1': principalDf.iloc[:,0].values,\n",
    "                   'X2': principalDf.iloc[:,1].values,\n",
    "                   'X3': principalDf.iloc[:,2].values,\n",
    "                   'X4': principalDf.iloc[:,3].values,\n",
    "                   'X5': principalDf.iloc[:,4].values,\n",
    "                   'Misclassified': testLabels != predictedTest})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate distance to turn image \"adversarial\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criticalClass = 0\n",
    "gradientSteps = boundaryMSE(criticalClass, testImages, predictedTest, model, boundarySteps)\n",
    "np.save('zapposBoundary1000', gradientSteps) \n",
    "boundDist = np.load('zapposBoundary1000.npy')\n",
    "boundDist2 = np.log(boundDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for High Confidence Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000\n",
    "utilFeatures = df.iloc[:,3:8].values                             ## PCA features to calculate distances in derived feature space\n",
    "##\n",
    "## Subset to critical class\n",
    "##\n",
    "##  (boundDist already subsetted to critical class)\n",
    "## \n",
    "utilFeatures = utilFeatures[predictedTest == criticalClass]\n",
    "confidence = confidenceTest[predictedTest == criticalClass]\n",
    "trueLabels = testLabels[predictedTest == criticalClass]\n",
    "predictedLabels = predictedTest[predictedTest == criticalClass]\n",
    "\n",
    "##\n",
    "## Subset to high confidence\n",
    "##\n",
    "utilFeatures = utilFeatures[confidence > 0.65]\n",
    "boundDist = boundDist2[confidence > 0.65]\n",
    "trueLabels = trueLabels[confidence > 0.65]\n",
    "predictedLabels = predictedLabels[confidence > 0.65]\n",
    "confidence = confidence[confidence > 0.65]\n",
    "\n",
    "##\n",
    "## Hold SDR utility values\n",
    "##\n",
    "sdrRand = np.zeros(iterations*toSelect).reshape(iterations, toSelect)     ## Random search\n",
    "sdrCon = np.zeros(iterations*toSelect).reshape(iterations, toSelect)      ## Low confidence search\n",
    "sdrLow = np.zeros(iterations*toSelect).reshape(iterations, toSelect)      ## Adversarial distance search\n",
    "sdrBW = np.zeros(iterations*toSelect).reshape(iterations, toSelect)       ## Bansal Weld search\n",
    "sdrLak = np.zeros(iterations*toSelect).reshape(iterations, toSelect)      ## Lakaraju search\n",
    "\n",
    "##\n",
    "## Hold Bansal and Weld utility values\n",
    "##\n",
    "bwRand = np.zeros(iterations*toSelect).reshape(iterations, toSelect)     ## Random search\n",
    "bwCon = np.zeros(iterations*toSelect).reshape(iterations, toSelect)      ## Low confidence search\n",
    "bwLow = np.zeros(iterations*toSelect).reshape(iterations, toSelect)      ## Adversarial distance search\n",
    "bwBW = np.zeros(iterations*toSelect).reshape(iterations, toSelect)       ## Bansal Weld search\n",
    "bwLak = np.zeros(iterations*toSelect).reshape(iterations, toSelect)      ## Lakaraju search\n",
    "\n",
    "##\n",
    "## Hold Lakaraju utility values s\n",
    "##\n",
    "fixedRand = np.zeros(iterations*toSelect).reshape(iterations, toSelect)  ## Random search\n",
    "fixedCon = np.zeros(iterations*toSelect).reshape(iterations, toSelect)   ## Low confidence search\n",
    "fixedLow = np.zeros(iterations*toSelect).reshape(iterations, toSelect)   ## Adversarial distance search\n",
    "fixedBW = np.zeros(iterations*toSelect).reshape(iterations, toSelect)    ## Bansal Weld search\n",
    "fixedLak = np.zeros(iterations*toSelect).reshape(iterations, toSelect)   ## Lakaraju search\n",
    "\n",
    "##\n",
    "## Hold spread values\n",
    "##\n",
    "spreadRand = np.zeros(iterations*toSelect).reshape(iterations, toSelect) ## Random search\n",
    "spreadCon = np.zeros(iterations*toSelect).reshape(iterations, toSelect)  ## Low confidence search\n",
    "spreadLow = np.zeros(iterations*toSelect).reshape(iterations, toSelect)  ## Adversarial distance search\n",
    "spreadBW = np.zeros(iterations*toSelect).reshape(iterations, toSelect)   ## Bansal Weld search\n",
    "spreadLak = np.zeros(iterations*toSelect).reshape(iterations, toSelect)  ## Lakaraju search\n",
    "\n",
    "##\n",
    "## Hold confidence of sampled points\n",
    "##\n",
    "conRand = np.zeros(iterations*toSelect).reshape(iterations, toSelect)    ## Random search\n",
    "conCon = np.zeros(iterations*toSelect).reshape(iterations, toSelect)     ## Low confidence search\n",
    "conLow = np.zeros(iterations*toSelect).reshape(iterations, toSelect)     ## Adversarial distance search\n",
    "conBW = np.zeros(iterations*toSelect).reshape(iterations, toSelect)      ## Bansal Weld search\n",
    "conLak = np.zeros(iterations*toSelect).reshape(iterations, toSelect)     ## Lakaraju search\n",
    "\n",
    "##\n",
    "## Hold indicator if sampled point was correctly predicted\n",
    "##\n",
    "uuRand = np.zeros(iterations*toSelect).reshape(iterations, toSelect)    ## Random search\n",
    "uuCon = np.zeros(iterations*toSelect).reshape(iterations, toSelect)     ## Low confidence search\n",
    "uuLow = np.zeros(iterations*toSelect).reshape(iterations, toSelect)     ## Adversarial distance search\n",
    "uuBW = np.zeros(iterations*toSelect).reshape(iterations, toSelect)      ## Bansal Weld search\n",
    "uuLak = np.zeros(iterations*toSelect).reshape(iterations, toSelect)     ## Lakaraju search\n",
    "\n",
    "##\n",
    "## Get distance matrix\n",
    "##\n",
    "distances = distMatrix(principalComponents)\n",
    "np.save('catDogDistance', distances) \n",
    "\n",
    "for i in range(iterations):\n",
    "    print(\"\\r\" + str(i) + \" of \" + str(iterations), end=\"\")\n",
    "    ##\n",
    "    ## Subset evaluation set and relevant information\n",
    "    ##\n",
    "    subset = random.sample(range(len(confidence)), sampleSize)                         ## Get random subset of evaluation dataset (changes initial conditions of search)\n",
    "    utilFeaturesSubset = utilFeatures[subset]\n",
    "    trueLabelsSubset = trueLabels[subset]\n",
    "    predictedLabelsSubset = predictedLabels[subset]\n",
    "    confidenceSubset = confidence[subset]\n",
    "    advDistanceSubset = adversarialDistanceLOESS(boundDist[subset], confidenceSubset)  ## Calculate adversarial distance\n",
    "    \n",
    "    ##\n",
    "    ## Perform searches\n",
    "    ##\n",
    "    rand = randSearch(len(confidenceSubset), toSelect)                                 ## Low confidence search\n",
    "    con = lowConfidence(confidenceSubset, toSelect)                                    ## Low confidence search\n",
    "    low = lowAdversarialDistance(advDistanceSubset, toSelect, confidenceSubset, 2.2)\n",
    "    high = highAdversarialDistance(advDistanceSubset, toSelect)\n",
    "   \n",
    "    ##\n",
    "    ## To run and evaluate code from Bansal Weld\n",
    "    ##\n",
    "    submod = SubmodUtilityModel(trueLabelsSubset, predictedLabelsSubset, confidenceSubset, var = 0.001)\n",
    "    submod.setup(utilFeaturesSubset)\n",
    "    \n",
    "    fixed = UtilityModel(utilFeaturesSubset, trueLabelsSubset, predictedLabelsSubset, confidenceSubset, np.zeros(len(confidenceSubset)), gamma = 0.0)\n",
    "    \n",
    "    clusters = cluster('kmeans_both', utilFeaturesSubset, 6, confidenceSubset)\n",
    "    k = len(np.unique(clusters))\n",
    "    bwSearch, utilities = adap_greedy('conf', submod, toSelect, 'cluster', k, clusters)\n",
    "    lakSearch, utilities = bandit_solution('uub', submod, k, clusters, toSelect)\n",
    "    \n",
    "    for j in range(0, toSelect):\n",
    "        sdrRand[i,j] = SDR(confidenceSubset[rand[:j+1]], predictedLabelsSubset[rand[:j+1]], trueLabelsSubset[rand[:j+1]])\n",
    "        sdrCon[i,j] = SDR(confidenceSubset[con[:j+1]], predictedLabelsSubset[con[:j+1]], trueLabelsSubset[con[:j+1]])\n",
    "        sdrLow[i,j] = SDR(confidenceSubset[low[:j+1]], predictedLabelsSubset[low[:j+1]], trueLabelsSubset[low[:j+1]])\n",
    "        sdrHigh[i,j] = SDR(confidenceSubset[high[:j+1]], predictedLabelsSubset[high[:j+1]], trueLabelsSubset[high[:j+1]])\n",
    "        sdrBW[i,j] = SDR(confidenceSubset[bwSearch[:j+1]], predictedLabelsSubset[bwSearch[:j+1]], trueLabelsSubset[bwSearch[:j+1]])\n",
    "        sdrLak[i,j] = SDR(confidenceSubset[lakSearch[:j+1]], predictedLabelsSubset[lakSearch[:j+1]], trueLabelsSubset[lakSearch[:j+1]])\n",
    "        \n",
    "        spreadRand[i,j] = spread(rand[:j+1], distances[np.ix_(subset,subset)])\n",
    "        spreadCon[i,j] = spread(con[:j+1], distances[np.ix_(subset,subset)])\n",
    "        spreadLow[i,j] = spread(low[:j+1], distances[np.ix_(subset,subset)])\n",
    "        spreadHigh[i,j] = spread(high[:j+1], distances[np.ix_(subset,subset)])\n",
    "        spreadBW[i,j] = spread(bwSearch[:j+1], distances[np.ix_(subset,subset)])\n",
    "        spreadLak[i,j] = spread(lakSearch[:j+1], distances[np.ix_(subset,subset)])\n",
    "      \n",
    "        \n",
    "        bwRand[i,j] = submod.get_utility(rand[:j+1])\n",
    "        bwCon[i,j] = submod.get_utility(con[:j+1])\n",
    "        bwLow[i,j] = submod.get_utility(low[:j+1])\n",
    "        bwHigh[i,j] = submod.get_utility(high[:j+1])\n",
    "        bwBW[i,j] = submod.get_utility(bwSearch[:j+1])\n",
    "        bwLak[i,j] = submod.get_utility(lakSearch[:j+1])\n",
    "        \n",
    "        fixedRand[i,j] = fixed.get_utility(rand[:j+1])\n",
    "        fixedCon[i,j] = fixed.get_utility(con[:j+1])\n",
    "        fixedLow[i,j] = fixed.get_utility(low[:j+1])\n",
    "        fixedHigh[i,j] = fixed.get_utility(high[:j+1])\n",
    "        fixedBW[i,j] = fixed.get_utility(bwSearch[:j+1])\n",
    "        fixedLak[i,j] = fixed.get_utility(lakSearch[:j+1])\n",
    "        \n",
    "        conRand[i,j] = confidenceSubset[rand[j]]\n",
    "        conCon[i,j] = confidenceSubset[con[j]]\n",
    "        conLow[i,j] = confidenceSubset[low[j]]\n",
    "        conHigh[i,j] = confidenceSubset[high[j]]\n",
    "        conBW[i,j] = confidenceSubset[bwSearch[j]]\n",
    "        conLak[i,j] = confidenceSubset[lakSearch[j]]\n",
    "        \n",
    "        uuRand[i,j] = predictedLabelsSubset[rand[j]] != trueLabelsSubset[rand[j]]\n",
    "        uuCon[i,j] = predictedLabelsSubset[con[j]] != trueLabelsSubset[con[j]]\n",
    "        uuLow[i,j] = predictedLabelsSubset[low[j]] != trueLabelsSubset[low[j]]\n",
    "        uuHigh[i,j] = predictedLabelsSubset[high[j]] != trueLabelsSubset[high[j]]\n",
    "        uuBW[i,j] = predictedLabelsSubset[bwSearch[j]] != trueLabelsSubset[bwSearch[j]]\n",
    "        uuLak[i,j] = predictedLabelsSubset[lakSearch[j]] != trueLabelsSubset[lakSearch[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "df1 = pd.DataFrame({'phi': 'BansalWeld',\n",
    "                    'iteration': [i for i, j in itertools.product(range(sdrBW.shape[0]), range(sdrBW.shape[1]))],\n",
    "                    'b': [j for i, j in itertools.product(range(sdrBW.shape[0]), range(sdrBW.shape[1]))],\n",
    "                    'SDR': [sdrBW[i,j] for i, j in itertools.product(range(sdrBW.shape[0]), range(sdrBW.shape[1]))],\n",
    "                    'spread': [spreadBW[i,j] for i, j in itertools.product(range(sdrBW.shape[0]), range(sdrBW.shape[1]))],\n",
    "                    'BW':[bwBW[i,j] for i, j in itertools.product(range(sdrBW.shape[0]), range(sdrBW.shape[1]))],\n",
    "                    'Fixed':[fixedBW[i,j] for i, j in itertools.product(range(sdrBW.shape[0]), range(sdrBW.shape[1]))],\n",
    "                    'Confidence':[conBW[i,j] for i, j in itertools.product(range(conBW.shape[0]), range(conBW.shape[1]))],\n",
    "                    'UnknownUnknown':[uuBW[i,j] for i, j in itertools.product(range(uuBW.shape[0]), range(uuBW.shape[1]))]})\n",
    "\n",
    "df2 = pd.DataFrame({'phi': 'Lakkaraju',\n",
    "                    'iteration': [i for i, j in itertools.product(range(sdrLak.shape[0]), range(sdrLak.shape[1]))],\n",
    "                    'b': [j for i, j in itertools.product(range(sdrLak.shape[0]), range(sdrLak.shape[1]))],\n",
    "                    'SDR': [sdrLak[i,j] for i, j in itertools.product(range(sdrLak.shape[0]), range(sdrLak.shape[1]))],\n",
    "                    'spread': [spreadLak[i,j] for i, j in itertools.product(range(sdrLak.shape[0]), range(sdrLak.shape[1]))],\n",
    "                    'BW':[bwLak[i,j] for i, j in itertools.product(range(sdrLak.shape[0]), range(sdrLak.shape[1]))],\n",
    "                    'Fixed':[fixedLak[i,j] for i, j in itertools.product(range(sdrLak.shape[0]), range(sdrLak.shape[1]))],\n",
    "                    'Confidence':[conLak[i,j] for i, j in itertools.product(range(conLak.shape[0]), range(conLak.shape[1]))],\n",
    "                    'UnknownUnknown':[uuLak[i,j] for i, j in itertools.product(range(uuLak.shape[0]), range(uuLak.shape[1]))]})\n",
    "\n",
    "df3 = pd.DataFrame({'phi': 'lowConfidence',\n",
    "                    'iteration': [i for i, j in itertools.product(range(sdrCon.shape[0]), range(sdrCon.shape[1]))],\n",
    "                    'b': [j for i, j in itertools.product(range(sdrCon.shape[0]), range(sdrCon.shape[1]))],\n",
    "                    'SDR': [sdrCon[i,j] for i, j in itertools.product(range(sdrCon.shape[0]), range(sdrCon.shape[1]))],\n",
    "                    'spread': [spreadCon[i,j] for i, j in itertools.product(range(sdrCon.shape[0]), range(sdrCon.shape[1]))],\n",
    "                    'BW':[bwCon[i,j] for i, j in itertools.product(range(sdrCon.shape[0]), range(sdrCon.shape[1]))],\n",
    "                    'Fixed':[fixedCon[i,j] for i, j in itertools.product(range(sdrCon.shape[0]), range(sdrCon.shape[1]))],\n",
    "                    'Confidence':[conCon[i,j] for i, j in itertools.product(range(conCon.shape[0]), range(conCon.shape[1]))],\n",
    "                    'UnknownUnknown':[uuCon[i,j] for i, j in itertools.product(range(uuCon.shape[0]), range(uuCon.shape[1]))]})\n",
    "\n",
    "df4 = pd.DataFrame({'phi': 'random',\n",
    "                    'iteration': [i for i, j in itertools.product(range(sdrRand.shape[0]), range(sdrRand.shape[1]))],\n",
    "                    'b': [j for i, j in itertools.product(range(sdrRand.shape[0]), range(sdrRand.shape[1]))],\n",
    "                    'SDR': [sdrRand[i,j] for i, j in itertools.product(range(sdrRand.shape[0]), range(sdrRand.shape[1]))],\n",
    "                    'spread': [spreadRand[i,j] for i, j in itertools.product(range(sdrRand.shape[0]), range(sdrRand.shape[1]))],\n",
    "                    'BW':[bwRand[i,j] for i, j in itertools.product(range(sdrRand.shape[0]), range(sdrRand.shape[1]))],\n",
    "                    'Fixed':[fixedRand[i,j] for i, j in itertools.product(range(sdrRand.shape[0]), range(sdrRand.shape[1]))],\n",
    "                    'Confidence':[conRand[i,j] for i, j in itertools.product(range(conRand.shape[0]), range(conRand.shape[1]))],\n",
    "                    'UnknownUnknown':[uuRand[i,j] for i, j in itertools.product(range(uuRand.shape[0]), range(uuRand.shape[1]))]})\n",
    "\n",
    "\n",
    "df5 = pd.DataFrame({'phi': 'adversarial',\n",
    "                    'iteration': [i for i, j in itertools.product(range(sdrLow.shape[0]), range(sdrLow.shape[1]))],\n",
    "                    'b': [j for i, j in itertools.product(range(sdrLow.shape[0]), range(sdrLow.shape[1]))],\n",
    "                    'SDR': [sdrLow[i,j] for i, j in itertools.product(range(sdrLow.shape[0]), range(sdrLow.shape[1]))],\n",
    "                    'spread': [spreadLow[i,j] for i, j in itertools.product(range(sdrLow.shape[0]), range(sdrLow.shape[1]))],\n",
    "                    'BW':[bwLow[i,j] for i, j in itertools.product(range(sdrLow.shape[0]), range(sdrLow.shape[1]))],\n",
    "                    'Fixed':[fixedLow[i,j] for i, j in itertools.product(range(sdrLow.shape[0]), range(sdrLow.shape[1]))],\n",
    "                    'Confidence':[conLow[i,j] for i, j in itertools.product(range(conLow.shape[0]), range(conLow.shape[1]))],\n",
    "                    'UnknownUnknown':[uuLow[i,j] for i, j in itertools.product(range(uuLow.shape[0]), range(uuLow.shape[1]))]})\n",
    "\n",
    "toSave = pd.concat([df1, df2, df3, df4, df5])\n",
    "toSave.to_csv('resultsZappos.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
